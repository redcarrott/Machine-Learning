{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pickle import dump, load\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from os import listdir\n",
    "\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value= 0\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "GPU device not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2786fd07167c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found GPU at: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU device not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0m: GPU device not found"
     ]
    }
   ],
   "source": [
    "# check for GPU\n",
    "\n",
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_name = \"OutdoorSeating\"\n",
    "concat_filename = attribute_name+'_>49.csv'\n",
    "review_filename = attribute_name+'_review.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(length, vocab_size):\n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "    conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "    embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "    conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(0.5)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    # channel 3\n",
    "    inputs3 = Input(shape=(length,))\n",
    "    embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "    conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(0.5)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    # merge\n",
    "    merged = concatenate([flat1, flat2, flat3])\n",
    "    # interpretation\n",
    "    dense1 = Dense(10, activation='relu')(merged)\n",
    "    outputs = Dense(1, activation='sigmoid')(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "    # compile\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize\n",
    "#     plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    " \n",
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "    return max([len(s.split()) for s in lines])\n",
    " \n",
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    # pad encoded sequences\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business-level CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3154, 3)\n"
     ]
    }
   ],
   "source": [
    "# read in dataset\n",
    "res_concat = pd.read_csv(concat_filename)\n",
    "print(res_concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2523, 91141) (631, 91141)\n"
     ]
    }
   ],
   "source": [
    "# clean \n",
    "cleaned_text = res_concat.text.apply(clean_doc)\n",
    "\n",
    "# split into train and test\n",
    "trainLines, testLines, y_train, y_test = train_test_split(\n",
    "    cleaned_text, res_concat[attribute_name], test_size=0.2, random_state=0)\n",
    "\n",
    "# preprocess and create X_train and X_test\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "length = max_length(trainLines)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# encode data\n",
    "X_train = encode_text(tokenizer, trainLines, length)\n",
    "X_test = encode_text(tokenizer, testLines, length)\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "model = define_model(length, vocab_size)\n",
    "# model.load_weights(\"OutdoorSeating_concat_weights.best.hdf5\")\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "71/71 [==============================] - ETA: 0s - loss: 2.4469 - accuracy: 0.4969 \n",
      "Epoch 00001: val_accuracy improved from -inf to 0.43083, saving model to GoodForKids_concat_weights.best.hdf5\n",
      "71/71 [==============================] - 863s 12s/step - loss: 2.4469 - accuracy: 0.4969 - val_loss: 0.6936 - val_accuracy: 0.4308\n",
      "Epoch 2/10\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000 \n",
      "Epoch 00002: val_accuracy did not improve from 0.43083\n",
      "71/71 [==============================] - 889s 13s/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6935 - val_accuracy: 0.4308\n",
      "Epoch 3/10\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000 \n",
      "Epoch 00003: val_accuracy did not improve from 0.43083\n",
      "71/71 [==============================] - 887s 12s/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6937 - val_accuracy: 0.4308\n",
      "Epoch 4/10\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000 \n",
      "Epoch 00004: val_accuracy did not improve from 0.43083\n",
      "71/71 [==============================] - 873s 12s/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6935 - val_accuracy: 0.4308\n",
      "Epoch 5/10\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000 \n",
      "Epoch 00005: val_accuracy did not improve from 0.43083\n",
      "71/71 [==============================] - 870s 12s/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6936 - val_accuracy: 0.4308\n",
      "Epoch 6/10\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000 \n",
      "Epoch 00006: val_accuracy did not improve from 0.43083\n",
      "71/71 [==============================] - 865s 12s/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6936 - val_accuracy: 0.4308\n",
      "Epoch 7/10\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000 \n",
      "Epoch 00007: val_accuracy did not improve from 0.43083\n",
      "71/71 [==============================] - 866s 12s/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4308\n",
      "Epoch 8/10\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4850 \n",
      "Epoch 00008: val_accuracy improved from 0.43083 to 0.56917, saving model to GoodForKids_concat_weights.best.hdf5\n",
      "71/71 [==============================] - 865s 12s/step - loss: 0.6932 - accuracy: 0.4850 - val_loss: 0.6931 - val_accuracy: 0.5692\n",
      "Epoch 9/10\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4921 \n",
      "Epoch 00009: val_accuracy did not improve from 0.56917\n",
      "71/71 [==============================] - 863s 12s/step - loss: 0.6932 - accuracy: 0.4921 - val_loss: 0.6933 - val_accuracy: 0.4308\n",
      "Epoch 10/10\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4806 \n",
      "Epoch 00010: val_accuracy did not improve from 0.56917\n",
      "71/71 [==============================] - 860s 12s/step - loss: 0.6932 - accuracy: 0.4806 - val_loss: 0.6932 - val_accuracy: 0.4308\n"
     ]
    }
   ],
   "source": [
    "# create checkpoints and save the best weights\n",
    "filepath=attribute_name+\"_concat_weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# evaluate model on training dataset\n",
    "history = model.fit([X_train,X_train,X_train], np.array(y_train), validation_split=0.1,\n",
    "                    verbose=1, epochs=10, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 52.773374\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on test dataset dataset\n",
    "loss, acc = model.evaluate([X_test,X_test,X_test],np.array(y_test), verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review-level CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 2)\n"
     ]
    }
   ],
   "source": [
    "# read in dataset\n",
    "res_reviews = pd.read_csv(review_filename)\n",
    "print(res_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 540) (30000, 540)\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)          [(None, 540)]        0           []                               \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)          [(None, 540)]        0           []                               \n",
      "                                                                                                  \n",
      " input_12 (InputLayer)          [(None, 540)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_9 (Embedding)        (None, 540, 100)     6670200     ['input_10[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_10 (Embedding)       (None, 540, 100)     6670200     ['input_11[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_11 (Embedding)       (None, 540, 100)     6670200     ['input_12[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 537, 32)      12832       ['embedding_9[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)             (None, 535, 32)      19232       ['embedding_10[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)             (None, 533, 32)      25632       ['embedding_11[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 537, 32)      0           ['conv1d_9[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 535, 32)      0           ['conv1d_10[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 533, 32)      0           ['conv1d_11[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling1d_9 (MaxPooling1D)  (None, 268, 32)     0           ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling1d_10 (MaxPooling1D  (None, 267, 32)     0           ['dropout_10[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_11 (MaxPooling1D  (None, 266, 32)     0           ['dropout_11[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " flatten_9 (Flatten)            (None, 8576)         0           ['max_pooling1d_9[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_10 (Flatten)           (None, 8544)         0           ['max_pooling1d_10[0][0]']       \n",
      "                                                                                                  \n",
      " flatten_11 (Flatten)           (None, 8512)         0           ['max_pooling1d_11[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 25632)        0           ['flatten_9[0][0]',              \n",
      "                                                                  'flatten_10[0][0]',             \n",
      "                                                                  'flatten_11[0][0]']             \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 10)           256330      ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1)            11          ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 20,324,637\n",
      "Trainable params: 20,324,637\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# clean \n",
    "cleaned_text = res_reviews.text.apply(clean_doc)\n",
    "\n",
    "# split into train and test\n",
    "trainLines, testLines, y_train, y_test = train_test_split(\n",
    "    cleaned_text, res_reviews[attribute_name], test_size=0.3, random_state=seed_value)\n",
    "\n",
    "# preprocess and create X_train and X_test\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "length = max_length(trainLines)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# encode data\n",
    "X_train = encode_text(tokenizer, trainLines, length)\n",
    "X_test = encode_text(tokenizer, testLines, length)\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "model = define_model(length, vocab_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66702\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1969/1969 [==============================] - ETA: 0s - loss: 0.2559 - accuracy: 0.8904\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.90171, saving model to OutdoorSeating_review_weights.hdf5\n",
      "1969/1969 [==============================] - 507s 257ms/step - loss: 0.2559 - accuracy: 0.8904 - val_loss: 0.2375 - val_accuracy: 0.9017\n",
      "Epoch 2/10\n",
      " 193/1969 [=>............................] - ETA: 8:29 - loss: 0.1043 - accuracy: 0.9649"
     ]
    }
   ],
   "source": [
    "# create checkpoints and save the best weights\n",
    "filepath=\"OutdoorSeating_review_weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.load_weights(\"OutdoorSeating_review_model1_weights.hdf5\")\n",
    "\n",
    "# evaluate model on training dataset\n",
    "history = model.fit([X_train,X_train,X_train], np.array(y_train), validation_split=0.1,\n",
    "                    verbose=1, epochs=10, callbacks=callbacks_list)\n",
    "# history = model.fit([X_train,X_train,X_train], np.array(y_train), validation_data=([X_test,X_test,X_test],np.array(y_test)),\n",
    "#                     verbose=1, epochs=10, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test dataset dataset\n",
    "loss, acc = model.evaluate([X_test,X_test,X_test],np.array(y_test), verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test dataset dataset\n",
    "loss, acc = model.evaluate([X_train,X_train,X_train],np.array(y_train), verbose=0)\n",
    "print('Train Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create checkpoints and save the best weights\n",
    "filepath=\"OutdoorSeating_review_overfit_weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.load_weights(\"OutdoorSeating_review_weights.hdf5\")\n",
    "\n",
    "# evaluate model on training dataset\n",
    "history = model.fit([X_train,X_train,X_train], np.array(y_train), validation_data=([X_test,X_test,X_test],np.array(y_test)),\n",
    "                    verbose=1, epochs=10, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test dataset dataset\n",
    "loss, acc = model.evaluate([X_train,X_train,X_train],np.array(y_train), verbose=0)\n",
    "print('Train Accuracy: %f' % (acc*100))\n",
    "# evaluate model on test dataset dataset\n",
    "loss, acc = model.evaluate([X_test,X_test,X_test],np.array(y_test), verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
